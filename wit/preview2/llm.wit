// A WASI interface dedicated to performing inferencing for Large Language Models.
interface llm {
	/// A Large Language Model.
	type inferencing-model = string

	/// Inference request parameters
	record inferencing-params {
		/// The maximum tokens that should be inferred.
		///
		/// Note: the backing implementation may return less tokens.
		max-tokens: u32,
		repeat-penalty: float32,
		repeat-penalty-last-n-token-count: u32,
		temperature: float32,
		top-k: u32,
		top-p: float32
	}

	/// The set of errors which may be raised by functions in this interface
	variant error {
		model-not-supported,
		runtime-error(string),
		invalid-input(string)
	}

	/// An inferencing result
	// TODO: this should be a stream
	type inferencing-result = string

	/// Perform inferencing using the provided model and prompt with the given optional params
	infer: func(model: inferencing-model, prompt: string, params: option<inferencing-params>) -> result<inferencing-result, error>

	/// The model used for generating embeddings
	type embedding-model = string

	/// Generate embeddings for the supplied list of text
	generate-embeddings: func(model: embedding-model, text: list<string>) -> result<list<list<float32>>, error>
}
